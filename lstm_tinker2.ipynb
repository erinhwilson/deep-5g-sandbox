{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 8mers: 65536\n"
     ]
    }
   ],
   "source": [
    "# create all possible 8-mers\n",
    "seqs8 = [''.join(x) for x in product(['A','C','G','T'], repeat=8)]\n",
    "print('Total 8mers:',len(seqs8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for assigning scores to a particular DNA sequence\n",
    "score_dict = {\n",
    "    'A':20,\n",
    "    'C':17,\n",
    "    'G':14,\n",
    "    'T':11\n",
    "}\n",
    "def score_seqs(seqs):\n",
    "    '''Each seq is just the average of the letter scores wrt score_dict'''\n",
    "    data = []\n",
    "    for seq in seqs:\n",
    "        score = np.mean([score_dict[base] for base in seq])\n",
    "        data.append([seq,score])\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['seq','score'])\n",
    "    return df\n",
    "                  \n",
    "def score_seqs_motif(seqs):\n",
    "    '''\n",
    "    Each seq is the average of the letter scores wrt score_dict but if\n",
    "    it has a TAT it gets a +10 but if it has a GCG it gets a -10\n",
    "    '''\n",
    "    data = []\n",
    "    for seq in seqs:\n",
    "        score = np.mean([score_dict[base] for base in seq])\n",
    "        if 'TAT' in seq:\n",
    "            score += 10\n",
    "        if 'GCG' in seq:\n",
    "            score -= 10\n",
    "        data.append([seq,score])\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['seq','score'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAA</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAAAC</td>\n",
       "      <td>19.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAAAG</td>\n",
       "      <td>19.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAAAT</td>\n",
       "      <td>18.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAACA</td>\n",
       "      <td>19.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        seq   score\n",
       "0  AAAAAAAA  20.000\n",
       "1  AAAAAAAC  19.625\n",
       "2  AAAAAAAG  19.250\n",
       "3  AAAAAAAT  18.875\n",
       "4  AAAAAACA  19.625"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mer8 = score_seqs(seqs8)\n",
    "mer8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAA</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAAAC</td>\n",
       "      <td>19.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAAAG</td>\n",
       "      <td>19.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAAAT</td>\n",
       "      <td>18.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAACA</td>\n",
       "      <td>19.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65531</th>\n",
       "      <td>TTTTTTGT</td>\n",
       "      <td>11.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65532</th>\n",
       "      <td>TTTTTTTA</td>\n",
       "      <td>12.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65533</th>\n",
       "      <td>TTTTTTTC</td>\n",
       "      <td>11.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65534</th>\n",
       "      <td>TTTTTTTG</td>\n",
       "      <td>11.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65535</th>\n",
       "      <td>TTTTTTTT</td>\n",
       "      <td>11.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65536 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            seq   score\n",
       "0      AAAAAAAA  20.000\n",
       "1      AAAAAAAC  19.625\n",
       "2      AAAAAAAG  19.250\n",
       "3      AAAAAAAT  18.875\n",
       "4      AAAAAACA  19.625\n",
       "...         ...     ...\n",
       "65531  TTTTTTGT  11.375\n",
       "65532  TTTTTTTA  12.125\n",
       "65533  TTTTTTTC  11.750\n",
       "65534  TTTTTTTG  11.375\n",
       "65535  TTTTTTTT  11.000\n",
       "\n",
       "[65536 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mer8_motif = score_seqs_motif(seqs8)\n",
    "mer8_motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stuff into pytorch dataloaders\n",
    "mer8motif_train_dl,\\\n",
    "mer8motif_test_dl, \\\n",
    "mer8motif_train_df, \\\n",
    "mer8motif_test_df = u.build_dataloaders_single(mer8_motif,batch_size=11)\n",
    "# change to batch size 11 so I can figure out the dimension errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fed9ca1a1f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mer8motif_train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(mer8motif_train_dl.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None,verbose=False):\n",
    "    '''\n",
    "    Apply loss function to a batch of inputs. If no optimizer\n",
    "    is provided, skip the back prop step.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('\\n\\nloss batch ****')\n",
    "        print(\"xb shape:\",xb.shape)\n",
    "        print(\"yb shape:\",yb.shape)\n",
    "\n",
    "    xb_out = model(xb.float())\n",
    "    if verbose:\n",
    "        print(\"model out pre loss\", xb_out.shape)\n",
    "    loss = loss_func(xb_out, yb.float())\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    #print(\"lb returning:\",loss.item(), len(xb))\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, test_dl):\n",
    "    '''\n",
    "    Fit the model params to the training data, eval on unseen data.\n",
    "    Loop for a number of epochs and keep train of train and val losses \n",
    "    along the way\n",
    "    '''\n",
    "    # keep track of losses\n",
    "    train_losses = []    \n",
    "    test_losses = []\n",
    "    \n",
    "    # loops through epochs\n",
    "    for epoch in range(epochs):\n",
    "        #print(\"TRAIN\")\n",
    "        model.train()\n",
    "        ts = []\n",
    "        ns = []\n",
    "        # collect train loss; provide opt so backpropo happens\n",
    "        for xb, yb in train_dl:\n",
    "            t, n = loss_batch(model, loss_func, xb, yb, opt=opt)\n",
    "            ts.append(t)\n",
    "            ns.append(n)\n",
    "        train_loss = np.sum(np.multiply(ts, ns)) / np.sum(ns)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        #print(\"EVAL\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                # loop through test batches\n",
    "                # returns loss calc for test set batch size\n",
    "                # unzips into two lists\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in test_dl]\n",
    "                # Note: no opt provided, backprop won't happen\n",
    "            )\n",
    "        # Gets average MSE loss across all batches (may be of diff sizes, hence the multiply)\n",
    "        #print(\"losses\", losses)\n",
    "        #print(\"nums\", nums)\n",
    "        test_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, test_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "def run_model(train_dl,test_dl, model, lr=0.01, epochs=20):\n",
    "    '''\n",
    "    Given data and a model type, run dataloaders with MSE loss and SGD opt\n",
    "    '''\n",
    "    # define loss func and optimizer\n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr) \n",
    "    \n",
    "    # run the training loop\n",
    "    train_losses, test_losses = fit(epochs, model, loss_func, optimizer, train_dl, test_dl)\n",
    "    \n",
    "    #return model, train_losses, test_losses\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to build LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNA_LSTM(nn.Module):\n",
    "    def __init__(self,seq_len,hidden_dim=10):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden = None # when initialized, should be tuple of (hidden state, cell state)\n",
    "        \n",
    "        self.rnn = nn.LSTM(4, hidden_dim,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "            \n",
    "\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        # initialize hidden and cell states with 0s\n",
    "        self.hidden =  (torch.zeros(1, batch_size, self.hidden_dim), \n",
    "                        torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        return self.hidden\n",
    "        #hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "    \n",
    "\n",
    "    def forward(self, xb,verbose=False):\n",
    "        if verbose:\n",
    "            print(\"original xb.shape:\", xb.shape)\n",
    "            print(xb) # 11 x 32\n",
    "        \n",
    "        # make the one-hot nucleotide vectors group together\n",
    "        xb = xb.view(-1,self.seq_len,4) \n",
    "        if verbose:\n",
    "            print(\"re-viewed xb.shape:\", xb.shape) # >> 11 x 8 x 4\n",
    "            print(xb)\n",
    "\n",
    "        # ** Init hidden/cell states?? **\n",
    "        batch_size = xb.shape[0]\n",
    "        if verbose:\n",
    "            print(\"batch_size:\",batch_size)\n",
    "        (h,c) = self.init_hidden(batch_size)\n",
    "         \n",
    "        # *******\n",
    "        \n",
    "        lstm_out, self.hidden = self.rnn(xb, (h,c)) # should this get H and C?\n",
    "        if verbose:\n",
    "            #print(\"lstm_out\",lstm_out)\n",
    "            print(\"lstm_out shape:\",lstm_out.shape) # >> 11, 8, 10\n",
    "            print(\"lstm_out[-1] shape:\",lstm_out[-1].shape) # >> 8 x 10\n",
    "            print(\"lstm_out[-1][-1] shape:\",lstm_out[-1][-1].shape) # 10\n",
    "\n",
    "            print(\"hidden len:\",len(self.hidden)) # 2\n",
    "            print(\"hidden[0] shape:\", self.hidden[0].shape) # >> 1 x 11 x 10\n",
    "            print(\"hidden[0][-1] shape:\", self.hidden[0][-1].shape) # >> 11 X 10\n",
    "            print(\"hidden[0][-1][-1] shape:\", self.hidden[0][-1][-1].shape) # >> 10\n",
    "\n",
    "            print(\"*****\")\n",
    "            # These vectors should be the same, right?\n",
    "            A = lstm_out[-1][-1]\n",
    "            B = self.hidden[0][-1][-1]\n",
    "            print(\"lstm_out[-1][-1]:\",A)\n",
    "            print(\"self.hidden[0][-1][-1]\",B)\n",
    "            print(\"==?\", A==B)\n",
    "            print(\"*****\")\n",
    "        \n",
    "        # attempt to get the last layer from each last position of \n",
    "        # all seqs in the batch? IS this the right thing to get?\n",
    "        last_layer = lstm_out[:,-1,:] # This is 11X10... and it makes FC out 11X1, which is what I want?\n",
    "        #last_layer = lstm_out[-1][-1].unsqueeze(0) # this was [10X1]? led to FC outoput being [1]?\n",
    "        if verbose:\n",
    "            print(\"last layer:\", last_layer.shape)\n",
    "\n",
    "        out = self.fc(last_layer) \n",
    "        if verbose:\n",
    "            print(\"LSTM->FC out shape:\",out.shape)   \n",
    "                                                \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNA_LSTM(\n",
       "  (rnn): LSTM(4, 10, batch_first=True)\n",
       "  (fc): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = len(mer8motif_train_df['seq'].values[0])\n",
    "\n",
    "mer8motif_model_lstm = DNA_LSTM(seq_len)\n",
    "mer8motif_model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewilson6/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.120449012796217\n",
      "1 0.5644370661318066\n",
      "2 0.2616728325850459\n",
      "3 0.15103856184447925\n",
      "4 0.10665999610672727\n",
      "5 0.07707490203751499\n",
      "6 0.22454623091280423\n",
      "7 0.06064956730692331\n",
      "8 0.05526983230087712\n",
      "9 0.11212700311615176\n",
      "10 0.09519580356221956\n",
      "11 0.04957094306356723\n",
      "12 0.03402372083281669\n",
      "13 0.032571924564013414\n",
      "14 0.06015142847803474\n",
      "15 0.03031152778510874\n",
      "16 0.0589027988395023\n",
      "17 0.08019384578666697\n",
      "18 0.03782405983770325\n",
      "19 0.043076120334176356\n"
     ]
    }
   ],
   "source": [
    "train_losses,test_losses= run_model(\n",
    "    mer8motif_train_dl,\n",
    "    mer8motif_test_dl,\n",
    "    mer8motif_model_lstm, \n",
    "    lr=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_seq_pred(model, seqs, oracle):\n",
    "    '''Given some sequences, get the model's predictions '''\n",
    "    for dna in seqs:\n",
    "        s = torch.tensor(u.one_hot_encode(dna))\n",
    "        pred = model(s.float())\n",
    "        actual = oracle[dna]\n",
    "        diff = actual - pred.item()\n",
    "        print(f\"{dna}: {pred.item()} actual:{actual} ({diff})\")\n",
    "        \n",
    "def quick_test8(model):\n",
    "    seqs1 = ['AAAAAAAA', 'CCCCCCCC','GGGGGGGG','TTTTTTTT']\n",
    "    seqs2 = ['AACCAACA','CCGGCGCG','GGGTAAGG', 'TTTCGTTT','TGTAATAC']\n",
    "    seqsTAT = ['TATAAAAA','CCTATCCC','GTATGGGG','TTTATTTT']\n",
    "    seqsGCG = ['AAGCGAAA','CGCGCCCC','GGGCGGGG','TTGCGTTT']\n",
    "    TATGCG =  ['ATATGCGA','TGCGTATT']\n",
    "    \n",
    "    scoring = dict(mer8_motif[['seq','score']].values)\n",
    "\n",
    "    for seqs in [seqs1, seqs2, seqsTAT, seqsGCG, TATGCG]:\n",
    "        quick_seq_pred(model, seqs, scoring)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAAAAAA: 19.20769500732422 actual:20.0 (0.7923049926757812)\n",
      "CCCCCCCC: 17.158016204833984 actual:17.0 (-0.15801620483398438)\n",
      "GGGGGGGG: 13.617986679077148 actual:14.0 (0.38201332092285156)\n",
      "TTTTTTTT: 11.39276123046875 actual:11.0 (-0.39276123046875)\n",
      "\n",
      "AACCAACA: 18.69882583618164 actual:18.875 (0.17617416381835938)\n",
      "CCGGCGCG: 5.050249099731445 actual:5.5 (0.4497509002685547)\n",
      "GGGTAAGG: 15.031036376953125 actual:15.125 (0.093963623046875)\n",
      "TTTCGTTT: 12.184320449829102 actual:12.125 (-0.05932044982910156)\n",
      "TGTAATAC: 15.418512344360352 actual:15.5 (0.08148765563964844)\n",
      "\n",
      "TATAAAAA: 27.15608787536621 actual:27.75 (0.5939121246337891)\n",
      "CCTATCCC: 25.800508499145508 actual:25.875 (0.07449150085449219)\n",
      "GTATGGGG: 23.516021728515625 actual:24.0 (0.483978271484375)\n",
      "TTTATTTT: 21.531230926513672 actual:22.125 (0.5937690734863281)\n",
      "\n",
      "AAGCGAAA: 7.887281894683838 actual:8.125 (0.2377181053161621)\n",
      "CGCGCCCC: 6.319345474243164 actual:6.25 (-0.06934547424316406)\n",
      "GGGCGGGG: 4.545219898223877 actual:4.375 (-0.17021989822387695)\n",
      "TTGCGTTT: 2.6123437881469727 actual:2.5 (-0.11234378814697266)\n",
      "\n",
      "ATATGCGA: 16.026493072509766 actual:15.875 (-0.15149307250976562)\n",
      "TGCGTATT: 14.873615264892578 actual:13.625 (-1.2486152648925781)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quick_test8(mer8motif_model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>score</th>\n",
       "      <th>oh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAA</td>\n",
       "      <td>20.000</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAAAC</td>\n",
       "      <td>19.625</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAAAG</td>\n",
       "      <td>19.250</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAAAT</td>\n",
       "      <td>18.875</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAACA</td>\n",
       "      <td>19.625</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        seq   score                                                 oh\n",
       "0  AAAAAAAA  20.000  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...\n",
       "1  AAAAAAAC  19.625  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...\n",
       "2  AAAAAAAG  19.250  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...\n",
       "3  AAAAAAAT  18.875  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...\n",
       "4  AAAAAACA  19.625  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mer8_motif.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
