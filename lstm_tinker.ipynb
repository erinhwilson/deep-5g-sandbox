{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 8mers: 65536\n"
     ]
    }
   ],
   "source": [
    "# create all possible 8-mers\n",
    "seqs8 = [''.join(x) for x in product(['A','C','G','T'], repeat=8)]\n",
    "print('Total 8mers:',len(seqs8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for assigning scores to a particular DNA sequence\n",
    "score_dict = {\n",
    "    'A':20,\n",
    "    'C':17,\n",
    "    'G':14,\n",
    "    'T':11\n",
    "}\n",
    "def score_seqs(seqs):\n",
    "    '''Each seq is just the average of the letter scores wrt score_dict'''\n",
    "    data = []\n",
    "    for seq in seqs:\n",
    "        score = np.mean([score_dict[base] for base in seq])\n",
    "        data.append([seq,score])\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['seq','score'])\n",
    "    return df\n",
    "                  \n",
    "def score_seqs_motif(seqs):\n",
    "    '''\n",
    "    Each seq is the average of the letter scores wrt score_dict but if\n",
    "    it has a TAT it gets a +10 but if it has a GCG it gets a -10\n",
    "    '''\n",
    "    data = []\n",
    "    for seq in seqs:\n",
    "        score = np.mean([score_dict[base] for base in seq])\n",
    "        if 'TAT' in seq:\n",
    "            score += 10\n",
    "        if 'GCG' in seq:\n",
    "            score -= 10\n",
    "        data.append([seq,score])\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['seq','score'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAA</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAAAC</td>\n",
       "      <td>19.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAAAG</td>\n",
       "      <td>19.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAAAT</td>\n",
       "      <td>18.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAACA</td>\n",
       "      <td>19.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        seq   score\n",
       "0  AAAAAAAA  20.000\n",
       "1  AAAAAAAC  19.625\n",
       "2  AAAAAAAG  19.250\n",
       "3  AAAAAAAT  18.875\n",
       "4  AAAAAACA  19.625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mer8 = score_seqs(seqs8)\n",
    "mer8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAA</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAAAC</td>\n",
       "      <td>19.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAAAG</td>\n",
       "      <td>19.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAAAT</td>\n",
       "      <td>18.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAACA</td>\n",
       "      <td>19.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65531</th>\n",
       "      <td>TTTTTTGT</td>\n",
       "      <td>11.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65532</th>\n",
       "      <td>TTTTTTTA</td>\n",
       "      <td>12.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65533</th>\n",
       "      <td>TTTTTTTC</td>\n",
       "      <td>11.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65534</th>\n",
       "      <td>TTTTTTTG</td>\n",
       "      <td>11.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65535</th>\n",
       "      <td>TTTTTTTT</td>\n",
       "      <td>11.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65536 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            seq   score\n",
       "0      AAAAAAAA  20.000\n",
       "1      AAAAAAAC  19.625\n",
       "2      AAAAAAAG  19.250\n",
       "3      AAAAAAAT  18.875\n",
       "4      AAAAAACA  19.625\n",
       "...         ...     ...\n",
       "65531  TTTTTTGT  11.375\n",
       "65532  TTTTTTTA  12.125\n",
       "65533  TTTTTTTC  11.750\n",
       "65534  TTTTTTTG  11.375\n",
       "65535  TTTTTTTT  11.000\n",
       "\n",
       "[65536 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mer8_motif = score_seqs_motif(seqs8)\n",
    "mer8_motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stuff into pytorch dataloaders\n",
    "mer8motif_train_dl,\\\n",
    "mer8motif_test_dl, \\\n",
    "mer8motif_train_df, \\\n",
    "mer8motif_test_df = u.build_dataloaders_single(mer8_motif,batch_size=11)\n",
    "# change to batch size 11 so I can figure out the dimension errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f7e9e2f70a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mer8motif_train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(mer8motif_train_dl.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    '''\n",
    "    Apply loss function to a batch of inputs. If no optimizer\n",
    "    is provided, skip the back prop step.\n",
    "    '''\n",
    "    print('loss batch ****')\n",
    "    print(\"xb shape:\",xb.shape)\n",
    "    print(\"yb shape:\",yb.shape)\n",
    "\n",
    "    xb_out = model(xb.float())\n",
    "    print(\"model out pre loss\", xb_out.shape)\n",
    "    loss = loss_func(xb_out, yb.float())\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    #print(\"lb returning:\",loss.item(), len(xb))\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, test_dl):\n",
    "    '''\n",
    "    Fit the model params to the training data, eval on unseen data.\n",
    "    Loop for a number of epochs and keep train of train and val losses \n",
    "    along the way\n",
    "    '''\n",
    "    # keep track of losses\n",
    "    train_losses = []    \n",
    "    test_losses = []\n",
    "    \n",
    "    # loops through epochs\n",
    "    for epoch in range(epochs):\n",
    "        #print(\"TRAIN\")\n",
    "        model.train()\n",
    "        ts = []\n",
    "        ns = []\n",
    "        # collect train loss; provide opt so backpropo happens\n",
    "        for xb, yb in train_dl:\n",
    "            t, n = loss_batch(model, loss_func, xb, yb, opt=opt)\n",
    "            ts.append(t)\n",
    "            ns.append(n)\n",
    "        train_loss = np.sum(np.multiply(ts, ns)) / np.sum(ns)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        #print(\"EVAL\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                # loop through test batches\n",
    "                # returns loss calc for test set batch size\n",
    "                # unzips into two lists\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in test_dl]\n",
    "                # Note: no opt provided, backprop won't happen\n",
    "            )\n",
    "        # Gets average MSE loss across all batches (may be of diff sizes, hence the multiply)\n",
    "        #print(\"losses\", losses)\n",
    "        #print(\"nums\", nums)\n",
    "        test_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, test_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "def run_model(train_dl,test_dl, model, lr=0.01, epochs=20):\n",
    "    '''\n",
    "    Given data and a model type, run dataloaders with MSE loss and SGD opt\n",
    "    '''\n",
    "    # define loss func and optimizer\n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr) \n",
    "    \n",
    "    # run the training loop\n",
    "    train_losses, test_losses = fit(epochs, model, loss_func, optimizer, train_dl, test_dl)\n",
    "    \n",
    "    #return model, train_losses, test_losses\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to build LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNA_LSTM(nn.Module):\n",
    "    def __init__(self,seq_len,hidden_dim=10):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden = None # when initialized, should be tuple of (hidden state, cell state)\n",
    "        \n",
    "        self.rnn = nn.LSTM(4, hidden_dim,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "            \n",
    "\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        # initialize hidden and cell states with 0s\n",
    "        self.hidden =  (torch.zeros(1, batch_size, self.hidden_dim), \n",
    "                        torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        return self.hidden\n",
    "        #hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "    \n",
    "\n",
    "    def forward(self, xb):\n",
    "        print(\"original xb.shape:\", xb.shape)\n",
    "        print(xb) # 11 x 32\n",
    "        \n",
    "        # make the one-hot nucleotide vectors group together\n",
    "        xb = xb.view(-1,self.seq_len,4) \n",
    "        print(\"re-viewed xb.shape:\", xb.shape) # >> 11 x 8 x 4\n",
    "        print(xb)\n",
    "\n",
    "        # ** Init hidden/cell states?? **\n",
    "        batch_size = xb.shape[0]\n",
    "        print(\"batch_size:\",batch_size)\n",
    "        (h,c) = self.init_hidden(batch_size)\n",
    "         \n",
    "        # *******\n",
    "        \n",
    "        lstm_out, self.hidden = self.rnn(xb, (h,c)) # should this get H and C?\n",
    "        #print(\"lstm_out\",lstm_out)\n",
    "        print(\"lstm_out shape:\",lstm_out.shape) # >> 11, 8, 10\n",
    "        print(\"lstm_out[-1] shape:\",lstm_out[-1].shape) # >> 8 x 10\n",
    "        print(\"lstm_out[-1][-1] shape:\",lstm_out[-1][-1].shape) # 10\n",
    "        \n",
    "        print(\"hidden len:\",len(self.hidden)) # 2\n",
    "        print(\"hidden[0] shape:\", self.hidden[0].shape) # >> 1 x 11 x 10\n",
    "        print(\"hidden[0][-1] shape:\", self.hidden[0][-1].shape) # >> 11 X 10\n",
    "        print(\"hidden[0][-1][-1] shape:\", self.hidden[0][-1][-1].shape) # >> 10\n",
    "        \n",
    "        print(\"*****\")\n",
    "        # These vectors should be the same, right?\n",
    "        A = lstm_out[-1][-1]\n",
    "        B = self.hidden[0][-1][-1]\n",
    "        print(\"lstm_out[-1][-1]:\",A)\n",
    "        print(\"self.hidden[0][-1][-1]\",B)\n",
    "        print(\"==?\", A==B)\n",
    "        print(\"*****\")\n",
    "        \n",
    "        print()\n",
    "        print(\"what does this reshaping do?\")\n",
    "        print(\"lstm_out shape:\",lstm_out.shape)  # out.shape = (batch_size, seq_len, hidden_size) >> 11, 8, 10\n",
    "        out = lstm_out.contiguous().view(-1, self.hidden_dim) # out.shape = (seq_len, hidden_size) \n",
    "                                                              # THIS IS WRONG ^^ it's batch_size*seq_len?! \n",
    "                                                              #                       88 x 10\n",
    "        print(\"re-viewed lstm_out shape:\",out.shape) \n",
    "        \n",
    "        out = self.fc(out)                                   # this is now 88 X 1 instead of 11 X 1\n",
    "        print(\"LSTM->FC out shape:\",out.shape)               # This makes it break at the loss function\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNA_LSTM(\n",
       "  (rnn): LSTM(4, 10, batch_first=True)\n",
       "  (fc): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = len(mer8motif_train_df['seq'].values[0])\n",
    "\n",
    "mer8motif_model_lstm = DNA_LSTM(seq_len)\n",
    "mer8motif_model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss batch ****\n",
      "xb shape: torch.Size([11, 32])\n",
      "yb shape: torch.Size([11, 1])\n",
      "original xb.shape: torch.Size([11, 32])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.]])\n",
      "re-viewed xb.shape: torch.Size([11, 8, 4])\n",
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.]]])\n",
      "batch_size: 11\n",
      "lstm_out shape: torch.Size([11, 8, 10])\n",
      "lstm_out[-1] shape: torch.Size([8, 10])\n",
      "lstm_out[-1][-1] shape: torch.Size([10])\n",
      "hidden len: 2\n",
      "hidden[0] shape: torch.Size([1, 11, 10])\n",
      "hidden[0][-1] shape: torch.Size([11, 10])\n",
      "hidden[0][-1][-1] shape: torch.Size([10])\n",
      "*****\n",
      "lstm_out[-1][-1]: tensor([-0.0980,  0.1818,  0.0168,  0.1786, -0.2685, -0.2174, -0.1380, -0.0221,\n",
      "         0.1070,  0.0597], grad_fn=<SelectBackward>)\n",
      "self.hidden[0][-1][-1] tensor([-0.0980,  0.1818,  0.0168,  0.1786, -0.2685, -0.2174, -0.1380, -0.0221,\n",
      "         0.1070,  0.0597], grad_fn=<SelectBackward>)\n",
      "==? tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "*****\n",
      "\n",
      "what does this reshaping do?\n",
      "lstm_out shape: torch.Size([11, 8, 10])\n",
      "re-viewed lstm_out shape: torch.Size([88, 10])\n",
      "LSTM->FC out shape: torch.Size([88, 1])\n",
      "model out pre loss torch.Size([88, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewilson6/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([11, 1])) that is different to the input size (torch.Size([88, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (88) must match the size of tensor b (11) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-975ac5f3cca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_losses,test_losses= run_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmer8motif_train_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmer8motif_test_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmer8motif_model_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-82521fa8b853>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(train_dl, test_dl, model, lr, epochs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# run the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m#return model, train_losses, test_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-82521fa8b853>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, train_dl, test_dl)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# collect train loss; provide opt so backpropo happens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-82521fa8b853>\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, loss_func, xb, yb, opt)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mxb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model out pre loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (88) must match the size of tensor b (11) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "train_losses,test_losses= run_model(\n",
    "    mer8motif_train_dl,\n",
    "    mer8motif_test_dl,\n",
    "    mer8motif_model_lstm, \n",
    "    lr=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
